{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Oe2rpzUb2k57",
        "outputId": "4d480b8e-d14b-4123-95b4-fd5a44d4f098"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 116) (<ipython-input-1-a46f4a765478>, line 116)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a46f4a765478>\"\u001b[0;36m, line \u001b[0;32m116\u001b[0m\n\u001b[0;31m    data['stemmed_text_data'] = [\".join(filter(None, filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 116)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import TimeDistributed, GlobalAveragePooling2D, BatchNormalization\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling1D\n",
        "from keras.layers import Dropout, Flatten, Bidirectional, Dense, Activation, TimeDistributed\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from string import ascii_lowercase\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models import doc2vec\n",
        "from gensim.models import KeyedVectors\n",
        "import itertools, nltk, snowballstemmer, re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import spacy\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class LabeledSentence(doc2vec.LabeledSentence):\n",
        "    pass\n",
        "\n",
        "class LabeledLineSentence(object):\n",
        "    def __init__(self, sources):\n",
        "        self.sources = sources\n",
        "        flipped = {}\n",
        "        # make sure that keys are unique for key, value in sources.items():\n",
        "        for value in sources.values():\n",
        "            if value not in flipped:\n",
        "                flipped[value] = [key]\n",
        "            else:\n",
        "                raise Exception(\"Non-unique prefix encountered\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '%s' % item_no])\n",
        "\n",
        "    def to_array(self):\n",
        "        self.sentences = []\n",
        "        for source, prefix in self.sources.items():\n",
        "            with utils.smart_open(source) as fin:\n",
        "                for item_no, line in enumerate(fin):\n",
        "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '%s' % item_no]))\n",
        "        return self.sentences\n",
        "\n",
        "    def sentences_perm(self):\n",
        "        shuffled = list(self.sentences)\n",
        "        random.shuffle(shuffled)\n",
        "        return shuffled\n",
        "\n",
        "data = pd.read_csv(\"deceptive-opinion.csv\")\n",
        "data.head()\n",
        "data['polarity'] = np.where(data['polarity'] == 'positive', 1, 0)\n",
        "data['deceptive'] = np.where(data['deceptive'] == 'truthful', 1, 0)\n",
        "\n",
        "\n",
        "def create_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return [1, 1]\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return [1, 0]\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return [0, 1]\n",
        "    else:\n",
        "        return [0, 0]\n",
        "\n",
        "\n",
        "def specific_class(c):\n",
        "    if c['polarity'] == 1 and c['deceptive'] == 1:\n",
        "        return \"TRUE_POSITIVE\"\n",
        "    elif c['polarity'] == 1 and c['deceptive'] == 0:\n",
        "        return \"FALSE POSITIVE\"\n",
        "    elif c['polarity'] == 0 and c['deceptive'] == 1:\n",
        "        return \"TRUE_NEGATIVE\"\n",
        "    else:\n",
        "        return \"FALSE NEGATIVE\"\n",
        "\n",
        "\n",
        "data['final_class'] = data.apply(create_class, axis=1)\n",
        "data['given_class'] = data.apply(specific_class, axis=1)\n",
        "data\n",
        "Y = data['given_class']\n",
        "Y\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "encoded_Y\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "textData = pd.DataFrame(list(data['text']))\n",
        "stemmer = snowballstemmer.EnglishStemmer()\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "stop.extend(['may', 'also', 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'across', 'among', 'beside', 'however', 'yet', 'within'])\n",
        "stop.extend(list(ascii_lowercase))\n",
        "stoplist = stemmer.stemWords(stop)\n",
        "stoplist = set(stoplist)\n",
        "stop = set(sorted(stop + list(stoplist)))\n",
        "textData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_~1234567890\\'\\\\\\]', '', inplace=True, regex=True)\n",
        "wordlist = filter(None, \" \".join(list(set(list(itertools.chain(*textData[0].str.split('')))))).split(\"\"))\n",
        "\n",
        "data['stemmed_text_data'] = [\".join(filter(None, filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]\n",
        "minimum_count = 1\n",
        "str_frequencies = pd.DataFrame(list(Counter(filter(None, list(itertools.chain(*data['stemmed_text_data'].str.split(' '))))).items()), columns=['word', 'count'])\n",
        "low_frequency_words = set(str_frequencies[str_frequencies['count'] <= minimum_count]['word'])\n",
        "data['stemmed_text_data'] = [' '.join(filter(None, filter(lambda word: word not in low_frequency_words, line))) for line in data['stemmed_text_data'].str.split(' ')]\n",
        "data['stemmed_text_data'] = [\" \".join(stemmer.stemWords(re.sub(\"[!\"#%\\'()*+,-/:;<=>?@\\[\\]^_~{|}~1234567890\\\\]\", '', next_text).split(' '))) for next_text in data['stemmed_text_data']]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "w = re.compile(\"\\w+\", re.I)\n",
        "\n",
        "def label_sentences(df, input_point):\n",
        "  labeled_sentences = []\n",
        "  list_sen = []\n",
        "  for index, datapoint in df.iterrows():\n",
        "    tokenized_words = re.findall(w, datapoint[input_point].lower())\n",
        "    labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' % index]))\n",
        "    list_sen.append(tokenized_words)\n",
        "  return labeled_sentences, list_sen\n",
        "\n",
        "def train_doc2vec_model(labeled_sentences):\n",
        "  model = Doc2Vec(min_count=1, window=9, size=512, sample=6e-5, negative=5, workers=7)\n",
        "  model.build_vocab(labeled_sentences)\n",
        "  pretrained_weights = model.wv.syn0\n",
        "  vocab_size, embedding_size = pretrained_weights.shape\n",
        "  model.train(labeled_sentences, total_examples=vocab_size, epochs=400)\n",
        "  return model\n",
        "\n",
        "textData = data['stemmed_text_data'].to_frame().reset_index()\n",
        "sen, corpus = label_sentences(textData, 'stemmed_text_data')\n",
        "doc2vec_model = train_doc2vec_model(sen)\n",
        "doc2vec_model.save(\"doc2vec_model_opinion_corpus.d2v\")\n",
        "doc2vec_model = Doc2Vec.load(\"doc2vec_model_opinion_corpus.d2v\")\n",
        "\n",
        "tfidfl = TfidfVectorizer(tokenizer=lambda i: i, lowercase=False, ngram_range=(1, 1))\n",
        "result_train1 = tfidfl.fit_transform(corpus)\n",
        "tfidf2 = TfidfVectorizer(tokenizer=lambda i: i, lowercase=False, ngram_range=(1, 2))\n",
        "result_train2 = tfidf2.fit_transform(corpus)\n",
        "tfidf3 = TfidfVectorizer(tokenizer=lambda i: i, lowercase=False, ngram_range=(1, 3))\n",
        "result_train3 = tfidf3.fit_transform(corpus)\n",
        "\n",
        "# Truncated SVD with n_components set to 512 and n_iter set to 40 for reproducibility\n",
        "svd = TruncatedSVD(n_components=512, n_iter=40, random_state=34)\n",
        "\n",
        "# Applying SVD on train data transformed by tfidf1, tfidf2 and tfidf3\n",
        "tfidf_datal = svd.fit_transform(result_train1)\n",
        "tfidf_data2 = svd.fit_transform(result_train2)\n",
        "tfidf_data3 = svd.fit_transform(result_train3)\n",
        "\n",
        "# Load spaCy language model for English ('en')\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Assuming temp_textData is a DataFrame containing the text data\n",
        "temp_textData = pd.DataFrame(list(data['text']))\n",
        "\n",
        "# Initialize empty lists to store POS tags, tokens, dependency tags\n",
        "overall_pos_tags_tokens = []\n",
        "overall_pos = []\n",
        "overall_tokens = []\n",
        "overall_dep = []\n",
        "\n",
        "# Iterate over each text data point in temp_textData\n",
        "for i in range(1600):\n",
        "  doc = nlp(temp_textData[0][i])  # Assuming text data is in the first column (index 0)\n",
        "  given_pos_tags_tokens = []\n",
        "  given_pos = []\n",
        "  given_tokens = []\n",
        "  given_dep = []\n",
        "\n",
        "  # Iterate over each token in the processed document\n",
        "  for token in doc:\n",
        "    output = \"%s %s\" % (token.pos_, token.tag_)  # Combine POS tag and tag info\n",
        "    given_pos_tags_tokens.append(output)\n",
        "    given_pos.append(token.pos_)\n",
        "    given_tokens.append(token.tag_)\n",
        "    given_dep.append(token.dep_)\n",
        "\n",
        "  # Append the processed information for each data point\n",
        "  overall_pos_tags_tokens.append(given_pos_tags_tokens)\n",
        "  overall_pos.append(given_pos)\n",
        "  overall_tokens.append(given_tokens)\n",
        "  overall_dep.append(given_dep)\n",
        "\n",
        "# Create count vectorizer for POS tags, tokens, and dependency tags\n",
        "count = CountVectorizer(tokenizer=lambda i: i, lowercase=False)\n",
        "\n",
        "# Convert POS tag tokens, POS tags, tokens, and dependency tags into sparse matrices\n",
        "pos_tags_data = count.fit_transform(overall_pos_tags_tokens).todense()\n",
        "pos_data = count.fit_transform(overall_pos).todense()\n",
        "tokens_data = count.fit_transform(overall_tokens).todense()\n",
        "dep_data = count.fit_transform(overall_dep).todense()\n",
        "\n",
        "# MinMaxScaler for normalization\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize POS tag tokens, POS tags, tokens, and dependency tags data\n",
        "normalized_pos_tags_data = min_max_scaler.fit_transform(pos_tags_data)\n",
        "normalized_pos_data = min_max_scaler.fit_transform(pos_data)\n",
        "normalized_tokens_data = min_max_scaler.fit_transform(tokens_data)\n",
        "normalized_dep_data = min_max_scaler.fit_transform(dep_data)\n",
        "\n",
        "# Initialize final feature matrices for POS tags, tokens, etc. with zeros\n",
        "final_pos_tags_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_pos_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_tokens_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "final_dep_data = np.zeros(shape=(1600, 512)).astype(np.float32)\n",
        "\n",
        "# Fill the final feature matrices with normalized data\n",
        "final_pos_tags_data[:normalized_pos_tags_data.shape[0], :normalized_pos_tags_data.shape[1]] = normalized_pos_tags_data\n",
        "final_pos_data[:normalized_pos_data.shape[0], :normalized_pos_data.shape[1]] = normalized_pos_data\n",
        "final_tokens_data[:normalized_tokens_data.shape[0], :normalized_tokens_data.shape[1]]=normalized_tokens_data\n",
        "final_dep_data[:normalized_dep_data.shape[0],:normalized_dep_data.shape[1]]=normalized_dep_data\n",
        "\n",
        " maxlength = []\n",
        "for i in range(0, len(sen)):\n",
        "    maxlength.append(len(sen[i][0]))\n",
        "print(max(maxlength))\n",
        "\n",
        "def vectorize_comments(df, d2v_model):\n",
        "    y = []\n",
        "    comments = []\n",
        "    for i in range(0, df.shape[0]):\n",
        "        label = 'SENT_%s' % i\n",
        "        comments.append(d2v_model.docvecs[label])\n",
        "    df['vectorized_comments'] = comments\n",
        "    return df\n",
        "\n",
        "textData = vectorize_comments(textData, doc2vec_model)\n",
        "print(textData.head(2))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textData[\"vectorized_comments\"].T.tolist(),\n",
        "    dummy_y,\n",
        "    test_size=0.1,\n",
        "    random_state=56\n",
        ")\n",
        "\n",
        "X = np.array(textData[\"vectorized_comments\"].T.tolist()).reshape((1, 1600, 512))\n",
        "y = np.array(dummy_y).reshape((1600, 4))\n",
        "\n",
        "X_train2 = np.array(X_train).reshape((1, 1440, 512))\n",
        "y_train2 = np.array(y_train).reshape((1, 1440, 4))\n",
        "\n",
        "X_test2 = np.array(X_test).reshape((1, 160, 512))\n",
        "y_test2 = np.array(y_test).reshape((1, 160, 4))\n",
        "\n",
        "Xtemp = textData[\"vectorized_comments\"].T.tolist()\n",
        "ytemp = data['given_class']\n",
        "training_indices = []\n",
        "testing_indices = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "skf.get_n_splits(Xtemp, ytemp)\n",
        "\n",
        "for train_index, test_index in skf.split(Xtemp, ytemp):\n",
        "    count_i = 0  # Initialize counter for training data index\n",
        "    for i in train_index:\n",
        "        len1 = len(sen[i][0])  # Get the length of the current sentence\n",
        "        average_vector1 = np.zeros(512).astype(np.float32)  # Initialize averaging vectors\n",
        "        average_vector2 = np.zeros(512).astype(np.float32)\n",
        "        average_vector3 = np.zeros(512).astype(np.float32)\n",
        "\n",
        "        for j in range(max(maxlength) + 10):\n",
        "            if j < len1:\n",
        "                # Fill X_train3 with doc2vec vectors and update averaging vectors\n",
        "                X_train3[count_i, j, :] = doc2vec_model[sen[i][0][j]]\n",
        "                average_vector1 += result_train1[i, tfidfl.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n",
        "            elif j == len1:\n",
        "                X_train3[count_i, j, :] = tfidf_data1[i]  # Fill with tfidf data\n",
        "            elif j == len1 + 1:\n",
        "                X_train3[count_i, j:, :] = tfidf_data2[i]  # Fill with tfidf data\n",
        "            elif j == len1 + 2:\n",
        "                X_train3[count_i, j, :] = tfidf_data3[i]  # Fill with tfidf data\n",
        "            elif j == len1 + 3:\n",
        "                X_train3[count_i, j, :] = average_vector1  # Fill with averaged doc2vec vector\n",
        "            elif j == len1 + 4:\n",
        "            X_test3[count_i, j, :] = average_vector2  # Fill with averaged doc2vec vector\n",
        "            elif j == len1 + 5:\n",
        "                X_test3[count_i, j, :] = average_vector3  # Fill with averaged doc2vec vector\n",
        "            elif j == len1 + 6:\n",
        "                X_test3[count_i, j:, :] = final_pos_tags_data[i]  # Fill with POS tag features\n",
        "            elif j == len1 + 7:\n",
        "                X_test3[count_i, j:, :] = final_pos_data[i]  # Fill with POS features\n",
        "            elif j == len1 + 8:\n",
        "                X_test3[count_i, j:, :] = final_tokens_data[i]  # Fill with token features\n",
        "            elif j == len1 + 9:\n",
        "                X_test3[count_i, j:, :] = final_dep_data[i]  # Fill with dependency features\n",
        "            else:\n",
        "                X_test3[count_i, j, :] = empty_word  # Fill with padding\n",
        "            Y_test3[count_i, :] = dummy_y[i]  # Fill target labels\n",
        "        count_i += 1\n",
        "\n",
        "      return X_train3, X_test3, Y_train3, Y_test3\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=128, kernel_size=9, padding='same', activation='relu',\n",
        "                 input_shape=(max(maxlength)+10, 512)))  # Input layer with Conv1D\n",
        "model.add(Dropout(0.25))  # Regularization with Dropout\n",
        "model.add(MaxPooling1D(pool_size=2))  # Downsampling with MaxPooling1D\n",
        "model.add(Dropout(0.25))  # Regularization\n",
        "model.add(Conv1D(filters=128, kernel_size=7, padding='same', activation='relu'))  # Another Conv1D layer\n",
        "model.add(Dropout(0.25))  # Regularization\n",
        "model.add(MaxPooling1D(pool_size=2))  # Downsampling\n",
        "model.add(Dropout(0.25))  # Regularization\n",
        "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))  # Another Conv1D layer\n",
        "model.add(Dropout(0.25))  # Regularization\n",
        "model.add(Bidirectional(LSTM(50, dropout=0.25, recurrent_dropout=0.2)))  # Bidirectional LSTM layer\n",
        "model.add(Dense(4, activation='softmax'))  # Output layer with 4 classes\n",
        "\n",
        "# Compile the model with optimizer and loss function\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Initialize a list to store final accuracies\n",
        "final_accuracies = []\n",
        "\n",
        "# Perform 10-fold cross-validation\n",
        "for i in range(10):\n",
        "    filename = 'weights.best.from_scratch%s.hdf5' % i  # Filename for best weights\n",
        "    checkpointer = ModelCheckpoint(filepath=filename, save_best_only=True, verbose=1)  # Checkpoint callback\n",
        "\n",
        "    # Get training and testing data for the current fold\n",
        "    X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(i)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train3,\n",
        "              Y_train3,\n",
        "              epochs=10,\n",
        "              callbacks=[checkpointer],\n",
        "              validation_data=(X_test3, Y_test3),\n",
        "              batch_size=512)\n",
        "\n",
        "    # Load the best weights for this fold\n",
        "    model.load_weights(filename)\n",
        "\n",
        "    # Make predictions and calculate accuracy\n",
        "    predicted = np.rint(model.predict(X_test3))\n",
        "    accuracy = accuracy_score(Y_test3, predicted)\n",
        "    print(\"Accuracy for fold\", i, \":\", accuracy)\n",
        "    final_accuracies.append(accuracy)\n",
        "\n",
        "# Print the average accuracy across folds\n",
        "print(\"Average accuracy:\", sum(final_accuracies) / len(final_accuracies))\n",
        "Y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HjelaiCn2rKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}